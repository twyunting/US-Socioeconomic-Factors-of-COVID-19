---
title: 'Evaluating US Socioeconomic Factors of COVID Confirmed Cases'
subtitle: "Keywords: Web Scrapping, Linear Regression, COVID-19"
author: "Yunting Chiu, Chiyun Liu, Ana Lim"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    fig_caption: true
urlcolor: blue
linkcolor: red
---
# Install Packages
Install the required libraries

- library(broom) # convert analysis objects from R into tidy tibbles
- library(tidyverse) # tidy data
- library(psych) # EDA
- library(countrycode) # get the continents
- library(lubridate) # adjust the date variable
- library(usmap) # find out the US states
- library(readxl) # read MS excel file
- library(corrplot) #for visualization of correlation
- library(leaps)  # regsubsets
- library(car) # ncvTest
- library(corrplot) # multicollinearity plot
- library(performance) # multicollinearity table 
```{r include=FALSE}
library(broom) # convert analysis objects from R into tidy tibbles
library(tidyverse) # tidy data
library(psych) # EDA
library(countrycode) # get the continents
library(lubridate) # adjust the date variable
library(usmap) # find out the US states
library(readxl) # read MS excel file
library(corrplot) #for visualization of correlation
library(leaps)  # regsubsets
library(car) # ncvTest
library(corrplot) # multicollinearity plot
library(performance) # multicollinearity table 
```

# Web Scraping COVID-19 Data
- The data source is from [Johns Hopkins University](https://github.com/CSSEGISandData). Here is the [link](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series)

```{r}
url_in <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/"
```

## Data Scraping
    + time_series_covid19_confirmed_global.csv
    + time_series_covid19_deaths_global.csv
    + time_series_covid19_confirmed_US.csv
    + time_series_covid19_deaths_US.csv
```{r}
df <- tibble(file_names = c("time_series_covid19_confirmed_global.csv",
                            "time_series_covid19_deaths_global.csv",
                            "time_series_covid19_confirmed_US.csv",
                            "time_series_covid19_deaths_US.csv")) -> df
```

## Data Mapping
### Using Regex for mapping
```{r, warning=FALSE}
df %>%
  mutate(url = str_c(url_in, file_names, sep = "")) -> df
```
```{r}
df %>%
  mutate(data = map(url, ~read_csv(., na = ""))) -> df
```

```{r}
df %>%
  mutate(case_types = as.factor(str_extract(file_names, "[:alpha:]*_[gU][:alpha:]*"))) -> 
  df
# alpha = Any letter, [A-Za-z]
# reference: https://www.petefreitag.com/cheatsheets/regex/character-classes/
```
```{r}
df %>%
  dplyr::select(case_types, data) -> df
```

## Clean Data  
```{r}
df %>%
  mutate(vars = map(df$data, names)) -> df
# map(df$vars, ~unlist(.)[1:15]) for checking

fix_names <- function(df, pattern, rePattern){
  stopifnot(is.data.frame(df), is.character(pattern), is.character(rePattern))
  names(df) <- str_replace_all(names(df), pattern, rePattern)
  return(df)
}

df %>%
  mutate(data = map(data, ~fix_names(., "([ey])/", "\\1_")),
         data = map(data, ~fix_names(., "Admin2", "County")),
         data = map(data, ~fix_names(., "Long_", "Long")),
         data = map_if(data, str_detect(df$case_types, "US"),
                   ~dplyr::select(., -c("UID", "iso2", "iso3", 
                                 "code3", "FIPS", "Combined_Key"))),
         data = map_if(data, str_detect(df$case_types, "global"),
                      ~mutate(., County = "NA")),
         data = map_if(data, !str_detect(df$case_types, "deaths_US"),
                      ~mutate(., Population = 0)),
         data = map(data, ~unite(., "Country_State", 
                                 c("Country_Region", "Province_State"),
                                 remove = FALSE, na.rm = TRUE,
                                 sep = "_"))
         ) -> df

df %>%
  mutate(vars = map(df$data, names)) -> df # synchronize the vars correspondingly
# map(df$vars, ~unlist(.)) # for checking 
```

```{r}
df %>%
  mutate(data = map(data, ~pivot_longer(data = ., cols = contains("/"),
                                        names_to = "Date",
                                        values_to = "dailyValues",
                                        names_transform = list(Date = mdy)))
         ) -> df
# df$data <- map(df$data, names) # synchronize the vars correspondingly
# map(df$vars, ~unlist(.)) # for checking 

# crate a function to fix in type of Date
mdyDate <- function(df, varsDate){
  # stopifnot(is.data.frame(df), is.character(varsDate))
  df[[varsDate]] <- ymd(df[[varsDate]])
  return(df)
}

df %>%
  mutate(data = map(data, ~mdyDate(., "Date"))) -> df_long

# str(df_long) # check the data set
```

### Add Continents and fix NAs
```{r, warning = FALSE}
df_long %>%
  mutate(data = map(data, ~mutate(., Continent = countrycode(Country_Region,
                                               origin = "country.name",
                                               destination = "continent")))
         ) -> df_long
```

```{r}
df_long %>%
  mutate(data = map(data, ~mutate(., Continent = case_when(
                                               Country_Region == "Diamond Princess" ~ "Asia",
                                               Country_Region == "Kosovo" ~ "Americas",
                                               Country_Region == "MS Zaandam" ~ "Europe",
                                               TRUE ~ Continent)
                                  ))) -> df_long

map(df_long$data, ~unique(.$Continent))
```
### Unnest the Data Frames    
```{r}
# 1
df_long %>%
  unnest(cols = data) %>%
  ungroup() -> df_all

# 2
remove(df, df_long)

# 3
df_all %>%
  dplyr::select(-vars) -> df_all
```

### Get World Population Data
- source: [UN source](https://population.un.org/wpp/Download/Standard/CSV/)
```{r, warning=FALSE}
# 1
df_pop <- read_csv("../data/WPP2019_TotalPopulation.csv")
# summarize(df_pop, across(everything(), ~sum(is.na(.)))) # check NAs

# 2 
semi_join(df_pop, df_all, by = c("Location" = "Country_Region")) -> df_pop

# 3
df_pop %>% 
  mutate(rank_p = rank(-PopTotal, na.last = TRUE),
         rank_d = rank(-PopDensity, na.last = TRUE),
         PopTotal = (PopTotal*1000)) -> df_pop
```

```{r}
df_all %>%
  inner_join(df_pop, by = c("Country_Region" = "Location")) -> df_all
```

## Tidy Data

- Because COVID-19 data is a time series data, we only focus on 2020/01/22 - 2021/01/22 for our experiment.
```{r}
#df_all %>% 
  #filter(case_types == "confirmed_US") %>%
  #select(Date, Province_State, County, dailyValues) %>%
  #tail()

# extract one year
df_all %>%
  filter(case_types == "confirmed_US" & as_date(Date) <= as_date("2021-01-22") | case_types == "deaths_US" & as_date(Date) <= as_date("2021-01-22")) -> covid

names(covid)


```

### Find out each US state using usmap
```{r}
state_map <- us_map(regions = "states")
state_map %>%
  distinct(full) %>%
  rename("Province_State" = "full") -> USstates
```

### Obtain the number of confirmed cases for each state
```{r}
covid %>%
  filter(case_types == "confirmed_US" & as_date(Date) == as_date("2021-01-22")) %>%
  dplyr::select(Province_State, County, dailyValues) %>%
  group_by(Province_State) %>%
  tally(dailyValues) %>%
  right_join(USstates) %>%
  rename("confirmed" = "n") -> confirmed
```
### Obtain the number of death cases for each state
```{r}
covid %>%
  filter(case_types == "deaths_US" & as_date(Date) == as_date("2021-01-22")) %>%
   dplyr::select(Province_State, County, dailyValues) %>%
  group_by(Province_State) %>%
  tally(dailyValues) %>%
  right_join(USstates) %>%
  rename("deaths" = "n") -> deathes

full_join(confirmed, deathes) -> covid
```

### Read 2019 American community survey estimate by race by state
- Source: https://www.governing.com/now/State-Population-By-Race-Ethnicity-Data.html
```{r}
race <- read_csv("../data/2019_state_community_by_race.csv")
race %>% 
  rename(Province_State = State) -> race

covid %>% 
  left_join(race, by = "Province_State") %>%
  rename(American_Indian_and_Alaska_Native_alone = "American Indian and Alaska Native alone",
         Asian_alone = "Asian alone",
         Black_or_African_American_alone = "Black or African American alone",
         Native_Hawaiian_and_Other_Pacific_Islander_alone = "Native Hawaiian and Other Pacific Islander alone",
         Some_other_race_alone = "Some other race alone",
         Total_Population = "Total Population",
         Two_or_more_races = "Two or more races",
         White_alone = "White alone") -> covid

#race %>% 
  #anti_join(covidForRegression, by = "Province_State") #-->Check if there are some diff value of state

# The 2020 American Community Survey (ACS) 1-year estimates will be released on September 23, 2021.
# Since the 2020 survey does not yet release, we use the 2019 survey here 
```

### Read 2021 Household income

- This data is tided from[World Population Review](https://worldpopulationreview.com/state-rankings/median-household-income-by-state), and the original, and source is from [US Census](https://www.census.gov/library/visualizations/interactive/2019-median-household-income.html)

```{r}
householdIncome2021 <- read_csv("../data/MedianHouseholdIncome2021.csv")
householdIncome2021 

householdIncome2021  %>% 
  rename(Province_State = State) -> householdIncome2021

covid %>% 
  left_join(householdIncome2021, by = "Province_State") -> covid

# Recheck
#covid %>%
  #dplyr::select(Province_State, HouseholdIncome) %>%
   #dplyr::arrange(desc(HouseholdIncome))

```

# Exploratory Data Analysis 
## Data Analysis and Visualization

```{r}
str(covid)
covid %>% head()
```

### Top 5 Confirmed Covid-19 Cases (Jan. 22, 2020 to Jan. 22, 2021)
```{r}
covid %>%
   dplyr::select(1, 2) %>%
  arrange(desc(confirmed)) %>%
  head(5)
```

### Top 5 Death Cases (Jan. 22, 2020 to Jan. 22, 2021)
```{r}
covid %>%
   dplyr::select(1, 3) %>%
  arrange(desc(deaths)) %>%
  head(5)
```
### Total Population vs Top 5 Median Income
```{r}
covid %>%
  dplyr::arrange(desc(HouseholdIncome)) %>%
  head(5) %>%
  ggplot(aes(x = Total_Population, y = HouseholdIncome, color = Province_State)) +
  geom_point() +
  theme_bw() +
  labs(x = "Total Population", y = "2021 Median Income", 
       title = "Total Population vs Top 5 Median Income")
```

In order to run the linear regression model smoothly, we deleted the `Province State`variable from our covid data and saved it as `covidForRegression` dataframe. The reason of removing these two is :

- The `Province State` variable has 51 different categorical types, and each observation has its own type, making it difficult to run a regression model.
- We refer to the existing paper[1](https://doi.org/10.1002/jmv.26095) & [2](https://ssrn.com/abstract=3612850) and select some similar independent variables as predictors.

```{r}
covid %>%
  dplyr::select(-Province_State) -> covidForRegression
names(covidForRegression)
```

## Descriptive Statistics
```{r}
describe(covidForRegression) 
```

# Linear Model Assumptions
## Multicollinearity
### Plot
```{r}
correlation <- cor(covidForRegression) 
corrplot(correlation, method="color", addCoef.col = "black", number.cex = 0.5, type = "lower")
```

### Table
Check the inversely related values of Tolerance and VIF. **Tolerance has to be > 0.10 and VIF < 10.** If these stipulated are not fulfilled, multicollinearity is at hand.

```{r}
reg <- lm(confirmed ~ ., data = covidForRegression)
check_collinearity(reg)
```
### Remediation - Removing Highly Correlated Predictors

- Remove highly correlated predictors from the model. If you have two or more factors with a high VIF, remove one from the model.
- According to the result, we firstly remove the high correlation variables: `Some_other_race_alone`, `Total_Population`, and `Two_or_more_races`, and keep the majority of race variables.
- After we removed, now the independent variables have very low correlation with each other.
```{r}
covidForRegression %>%
  dplyr::select(-Total_Population, -Two_or_more_races, -Some_other_race_alone) -> covidForRegression2
names(covidForRegression2)
```

```{r}
reg <- lm(confirmed ~ ., data = covidForRegression2)
check_collinearity(reg)
```


## Independence

We would need to know more from the data providers to really assess this. We will assume it holds.

## Linearity
The lack of fit F test works only with **simple linear regression** so we see the residual plots. As for the residuals versus fitted plot below, there may be no pattern indicating non-linearity in the data, but we attempt to remove some potential outliers.

```{r}
par(mfrow = c(2,2))
# summary(reg)
plot(reg)
```

### Remediation - Removing Influential Outliers
- Get other diagnostics measures

Take away the largest values of **cooks** and **leverage**. That is, observations 5 (California), is no longer present.
```{r}
leverage <- hatvalues(reg)
student <- rstudent(reg)
dfs <- dffits(reg)
cooksd <- cooks.distance(reg)
data.frame(confirmed = covidForRegression2$confirmed, fitted = reg$fitted,
           residual = reg$residual, leverage, student, dffits = dfs, cooksd) -> diag

par(mfrow=c(2,2))
plot(leverage,type='h')
abline(h=0)
plot(student,type='h')
abline(h=0)
plot(dfs,type='h',ylab='dffit')
abline(h=0)
plot(cooksd,type='h')
abline(h=0)

diag %>%
  arrange(desc(leverage)) %>%
  head(3)

diag %>%
  arrange(desc(cooksd)) %>%
  head(3)
```

- Look at the Residual vs Fitted plot; linearity has occurred.
```{r}
reg1 <- lm(confirmed ~ ., data = covidForRegression2[-5,])
par(mfrow=c(2,2))
plot(reg1)
```

## Homoscedasticity
With a small p-value, we have evidence that the variances are non-constant 
```{r}
# test homoscedasticity
ncvTest(reg1)
```
### Remediation - Square Root of Y
With a high p-value of 0.84229, there is no evidence of non-constant variance.
```{r}
covidForRegression2 %>%
  mutate(confirmed = sqrt(confirmed)) -> covidForRegressionSQRT

reg2 <- lm(confirmed ~ ., data = covidForRegressionSQRT[-5,])
ncvTest(reg2)
```

## Normality
- The p-value of the Shapiro-Wilk Test 0.1822 is greater than $\alpha$ 0.05 so the data is follow a normal distribution. 
```{r}
# test normality
shapiro.test(rstudent(reg2))
```


# Linear Model Selection

Our final mission is to select the **fewest** predictors and the determine by the **lowest** mean squared error in the linear model.

Adding all variables as full model first. We can see that only three variables in the linear model are significant at the beginning, which is `deaths `, `Black_or_African_American_alone`, and  `White_alone`.

```{r}
summary(reg2)
```

## Exhaustive Search
Using algorithm to select the best model exhaustively. Also, to use all X-variables available, change the `nvmax option`. Because I am too lazy to count variables, I entered a much larger number, such as my favorite number `69` to do it.
```{r}
reg_fitExhaustive <- regsubsets(confirmed ~ ., data = covidForRegressionSQRT[-5,], nvmax = 69)
summary(reg_fitExhaustive)
```

Select the variables and choose the optimal model
```{r}
summary(reg_fitExhaustive)$adjr2 
summary(reg_fitExhaustive)$cp 
summary(reg_fitExhaustive)$bic

which.max(summary(reg_fitExhaustive)$adjr2)
which.min(summary(reg_fitExhaustive)$cp) 
which.min(summary(reg_fitExhaustive)$bic)
```


## Sequential Search
We can also choose the best model by means of a stepwise procedure, starting with one model and ending with another

### Forward Method
Forward addition can be used to perform variable selection.
```{r message=FALSE}
reg_fitForward <- regsubsets(confirmed ~ ., 
                             data = covidForRegressionSQRT[-c(5),], 
                             method = "forward")
summary(reg_fitForward)
```

Select the variables and choose the optimal model
```{r}
summary(reg_fitForward)$adjr2 
summary(reg_fitForward)$cp 
summary(reg_fitForward)$bic

which.max(summary(reg_fitForward)$adjr2)
which.min(summary(reg_fitForward)$cp) 
which.min(summary(reg_fitForward)$bic)

```

### Backward Method
Backward elimination can be used to perform variable selection.
```{r message=FALSE}
reg_fitBackward <- regsubsets(confirmed ~ ., 
                              data = covidForRegressionSQRT[-c(5),], method = "backward")
summary(reg_fitBackward)
```

Select the variables and choose the optimal model
```{r}
summary(reg_fitBackward)$adjr2 
summary(reg_fitBackward)$cp 
summary(reg_fitBackward)$bic

which.max(summary(reg_fitBackward)$adjr2)
which.min(summary(reg_fitBackward)$cp) 
which.min(summary(reg_fitBackward)$bic)

```

## Stepwise Method

We use algorithm to considers either adding or removing variables at each step to final the best model. Lower AIC (Akaike information criterion) values indicate a better-fit model.
```{r}
null = lm(confirmed ~ 1, data = covidForRegressionSQRT[-c(5),])
full = lm(confirmed ~ ., data = covidForRegressionSQRT[-c(5),]) 
step(null, scope = list(lower = null, upper = full), direction = "both")
```


## Comparison
We will select the fewest variable for each set, compare their MSE, and finally select the one with the local minimum MSE.

1. From exhaustive search, the fewest predictors is 4 in smallest BIC (Bayesian Information Criterion).
```{r}
regExhaustive <- lm(confirmed ~ deaths + Asian_alone + Black_or_African_American_alone + White_alone
                    , data = covidForRegressionSQRT[-c(5),])
summary(regExhaustive)
```

2. For forward method in sequential search, the fewest predictors is 2 in smallest BIC (Bayesian Information Criterion).
```{r}
regForward <- lm(confirmed ~ Black_or_African_American_alone + White_alone,
                 data = covidForRegressionSQRT[-c(5),])
summary(regForward)
```

3. For backward method in sequential search, the fewest predictors is 4 in smallest BIC (Bayesian Information Criterion).
```{r}
regBackward <-  lm(confirmed ~ deaths + Asian_alone + Black_or_African_American_alone + White_alone
                    , data = covidForRegressionSQRT[-c(5),])
summary(regBackward)
```

4. For stepwise method, the lowest AIC value is **462.9**, reporting the best model with 5 variables is: `lm(formula = confirmed ~ White_alone + Black_or_African_American_alone + deaths + Asian_alone + American_Indian_and_Alaska_Native_alone, data = covidForRegressionSQRT[-c(5), ])`.
```{r}
regStepwise <- lm(formula = confirmed ~ White_alone + Black_or_African_American_alone + 
                    deaths + Asian_alone + American_Indian_and_Alaska_Native_alone, 
                  data = covidForRegressionSQRT[-c(5), ])
summary(regStepwise)

```

5. Let us start to find the minimum MSE
```{r}
#calculate MSE
anova(regExhaustive) %>% tidy() # 9863.275
anova(regForward) %>% tidy() # 11211.07	
anova(regBackward) %>% tidy() # 9863.275	
anova(regStepwise) %>% tidy() # 9375.065	
```

## Final Selection
`regStepwise` has the smallest MSE value (9375.065) so we select it as the best model. Therefore, our final predictors of square root of COVID confirmed cases by US States is `deaths`, `White_alone`,`Black_or_African_American_alone`, `Asian_alone` and `American_Indian_and_Alaska_Native_alone`.
```{r}
finalReg <- lm(formula = confirmed ~ White_alone + Black_or_African_American_alone + 
    deaths + Asian_alone + American_Indian_and_Alaska_Native_alone, 
    data = covidForRegressionSQRT[-c(5), ])
summary(finalReg) 
```

# Model Equation
## Interpretation

With the large p-value 0.07426 of $b_5$, the prodictor `American_Indian_and_Alaska_Native_alone` is not in the significant level. In other words, we fail to reject the null (H0: $\beta_5$ = 0 cannot be rejected) so we can conclude $b_5$ is 0. Therefore, $b_5$ can be dropped from the model.

## Linear Equation

Thus, the expected value of square root of confirmed cases is:
$$
 \sqrt{\hat{Confirmed}} = 295.4 + 0.00004354White + 0.0000614Black + 0.01254Deaths - 0.0002163Asian
$$

# References
(1) Sehra, S., Fundin, S., Lavery, C., & Baker, J. (2020). Differences in race and other state‐level characteristics and associations with mortality from COVID‐19 infection. *Journal of Medical Virology, 92*(11), 2406–2408. https://doi.org/10.1002/jmv.26095

(2) Sa, Filipa G., Socioeconomic Determinants of Covid-19 Infections and Mortality: Evidence from England and Wales (May 2020). CEPR Discussion Paper No. DP14781, Available at SSRN: https://ssrn.com/abstract=3612850

